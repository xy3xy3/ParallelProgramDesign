# 中山大学计算机院本科生实验报告

（2025学年春季学期）

课程名称：并行程序设计
批改人：

|实验| 3-Pthreads并行矩阵乘法与数组求和 |专业（方向）|计算机科学与技术 |
|---|---|---|---|
|学号|22336087 |姓名|胡瑞康 |
|Email|hurk3@mail2.sysu.edu.cn |完成日期|2025.4.2 |


## 任务1. 并行矩阵乘法


### 代码介绍

本实验实现了两种使用 Pthreads 实现的并行矩阵乘法方案，分别为“**行划分**”（`row_pthread.cpp`）和“**块划分**”（`block_pthread.cpp`）。两种方法都遵循如下基本流程：

1. 读取输入参数（线程数与矩阵维度）；
2. 动态分配内存，随机初始化矩阵 A（m×n）和 B（n×k）；
3. 创建若干线程，分别并行计算结果矩阵 C 的不同部分；
4. 等待所有线程完成后，统计运行时间；
5. 若矩阵较小，则输出计算结果矩阵 C；
6. 释放所有内存资源。

#### 行划分版本（`row_pthread.cpp`）

本方案按**行粒度**将矩阵 A 的多行划分给不同线程，每个线程计算其负责的所有行在结果矩阵 C 中的对应值。

- **输入与初始化**

程序读取线程数和矩阵维度：

```cpp
scanf("%d %d %d %d", &thread_count, &m, &n, &k);
```

接着，动态分配矩阵 A、B、C，并使用 `rand()` 生成 0-9 范围的随机数初始化 A 和 B。

- **线程任务划分**

通过如下逻辑将 A 的所有行尽可能均匀地划分给多个线程，每个线程记录其 `start_row` 和 `end_row`：

```cpp
int rows_per_thread = m / thread_count;
int remainder = m % thread_count;
```

前 remainder 个线程每人多分到 1 行，保证任务平均。

- **线程计算逻辑**

每个线程调用 `thread_func`，对其负责的多行执行如下乘法计算：

```cpp
for (int i = targ->start_row; i < targ->end_row; i++) {
    for (int j = 0; j < k; j++) {
        double sum = 0.0;
        for (int l = 0; l < n; l++) {
            sum += A[IDX(i, l, n)] * B[IDX(l, j, k)];
        }
        C[IDX(i, j, k)] = sum;
    }
}
```

这段代码逻辑为经典的三重循环矩阵乘法：A 的第 i 行 与 B 的第 j 列做点积得到 C 的 i 行 j 列。

- **线程创建与同步**

主线程负责 `pthread_create()` 启动多个线程，并在之后使用 `pthread_join()` 等待所有线程完成。计算耗时使用 `gettimeofday()` 记录前后时间差。

- **输出与释放资源**

若矩阵规模较小（如维度均小于等于 10），则输出矩阵 A、B 与 C；否则仅打印计算耗时。最后手动释放所有动态内存。

#### 块划分版本（`block_pthread.cpp`）

块划分方法将结果矩阵 C 划分为若干小矩形块（二维子矩阵），每个线程负责一个块的计算。其实现主要在以下几个方面与行划分不同：

- **网格划分策略**

将线程数分解为 `row_blocks × col_blocks` 的二维布局：

```cpp
void compute_grid(...) {
    int r = sqrt(thread_count);
    while (r > 0) {
        if (thread_count % r == 0) {
            *row_blocks = r;
            *col_blocks = thread_count / r;
            return;
        }
        r--;
    }
}
```

这确保每个线程可以独立处理一个 `(block_row × block_col)` 的区域。

- **任务划分方式**

每个线程计算一个子块，其坐标范围通过 `row_start ~ row_end` 与 `col_start ~ col_end` 指定。例如：

```cpp
args[thread_id].row_start = current_row;
args[thread_id].row_end = current_row + block_rows;
args[thread_id].col_start = current_col;
args[thread_id].col_end = current_col + block_cols;
```

这种二维划分方式在理论上可以更好地利用 CPU 缓存局部性，特别是在多核多级缓存系统下。

- **计算逻辑**

线程内部的计算逻辑与行划分几乎相同，只是行列范围为对应的块边界：

```cpp
for (int i = targ->row_start; i < targ->row_end; i++) {
    for (int j = targ->col_start; j < targ->col_end; j++) {
        ...
    }
}
```

- **线程创建顺序**

线程是以二维嵌套循环方式创建（每个块对应一个线程），而非按行顺序。


### 运行测试

使用修改版的`evaluate.py`脚本进行自动化测试，这里仅仅展示线程数和矩阵规模
```python
implementations = {
    "行划分": {
        "source": "row_pthread.cpp",
        "binary": "row_exec",
        "thread_counts": [1, 2, 4, 8, 16],
        "matrix_sizes": [128, 256, 512, 1024, 2048]
    },
    "块划分": {
        "source": "block_pthread.cpp",
        "binary": "block_exec",
        "thread_counts": [1, 2, 4, 8, 16],
        "matrix_sizes": [128, 256, 512, 1024, 2048]
    }
}
```


### 表格展示

行划分 - 测试结果表格（单位：秒）

|线程数|128|256|512|1024|2048|
| :-: | :-: | :-: | :-: | :-: | :-: |
|1|0.002228|0.029287|0.752014|6.345801|90.402978|
|2|0.001246|0.013822|0.355997|3.088190|52.618956|
|4|0.000960|0.007432|0.178382|1.452069|27.435417|
|8|0.000911|0.003992|0.081713|0.696357|15.473034|
|16|0.001144|0.003401|0.058410|0.377127|11.676839|

---

块划分 - 测试结果表格（单位：秒）

|线程数|128|256|512|1024|2048|
| :-: | :-: | :-: | :-: | :-: | :-: |
|1|0.002073|0.026610|0.742465|7.509419|92.433633|
|2|0.001693|0.013117|0.347714|3.021394|51.060233|
|4|0.001047|0.008375|0.177453|1.452103|26.221259|
|8|0.000725|0.003941|0.087281|0.782877|15.235888|
|16|0.001166|0.003471|0.050310|0.383902|11.370524|

### 表格分析

1. 小矩阵(128-256)：
   - 线程数较少时块划分表现更好（缓存局部性优势）
   - 线程数增加后行划分更优（任务分配开销更低）

2. 中等矩阵(512-1024)：
   - 两种方法性能差异缩小（<5%）
   - 块划分在16线程时优势明显（最高快16%）

3. 大矩阵(2048)：
   - 块划分全面占优（并行效率更高）
   - 16线程时块划分比行划分快3%

4. 线程扩展性：
   - 两种方法都展现良好的多线程加速
   - 块划分在高线程数时扩展性更优

整体来说，两种方法结果差别不大。


## 任务2. 并行数组求和

### 代码介绍

本实验实现了两种使用 Pthreads 实现的并行数组求和方案，分别为"**Mutex聚合**"（`sum_pthread_mutex.cpp`）和"**局部聚合**"（`sum_pthread_local.cpp`）。两种方法都遵循如下基本流程：

1. 读取输入参数（线程数与数组长度）；
2. 动态分配内存，随机初始化数组（取值0-9）；
3. 创建若干线程，分别并行计算数组的不同部分和；
4. 聚合部分和得到最终结果；
5. 统计运行时间并输出结果；
6. 释放所有内存资源。

#### 公共部分

- **输入与初始化**

  程序读取线程数和数组长度：
  ```cpp
  scanf("%d %d", &thread_count, &n);
  ```
  动态分配数组内存，并使用 `rand()` 生成 0-9 范围的随机数初始化数组。

- **任务划分**

  将数组尽可能均匀地划分给多个线程：
  ```cpp
  int chunk = n / thread_count;
  int remainder = n % thread_count;
  ```
  前 remainder 个线程每人多分到 1 个元素，保证任务平均。

#### Mutex聚合版本（`sum_pthread_mutex.cpp`）

- **关键机制**

  使用互斥锁（mutex）保护全局和变量：
  ```cpp
  pthread_mutex_t mutex;
  long long global_sum = 0;
  ```

- **线程计算逻辑**

  每个线程先计算自己的部分和，然后通过互斥锁安全地更新全局和：
  ```cpp
  void* thread_func(void* arg) {
      // 计算部分和
      long long partial_sum = 0;
      for (int i = targ->start; i < targ->end; i++) {
          partial_sum += A[i];
      }
      // 加锁更新全局和
      pthread_mutex_lock(&mutex);
      global_sum += partial_sum;
      pthread_mutex_unlock(&mutex);
      return NULL;
  }
  ```

- **同步机制**

  主线程通过 `pthread_join` 等待所有线程完成，无需额外的结果聚合步骤。

#### 局部聚合版本（`sum_pthread_local.cpp`）

- **关键机制**

  使用线程局部存储（每个线程独立的部分和数组）：
  ```cpp
  long long *partial_sums; // 每个线程一个元素
  ```

- **线程计算逻辑**

  每个线程将结果存入自己的部分和槽位：
  ```cpp
  void* thread_func(void* arg) {
      long long sum = 0;
      for (int i = targ->start; i < targ->end; i++) {
          sum += A[i];
      }
      partial_sums[targ->thread_id] = sum;
      return NULL;
  }
  ```

- **结果聚合**

  主线程在所有线程完成后，累加部分和数组：
  ```cpp
  long long total_sum = 0;
  for (int i = 0; i < thread_count; i++) {
      total_sum += partial_sums[i];
  }
  ```

### 运行测试

使用 `evaluate2.py` 脚本进行自动化测试，测试配置如下：

```python
implementations = {
    "Mutex聚合": {
        "source": "sum_pthread_mutex.cpp",
        "binary": "sum_mutex_exec",
        "thread_counts": [1, 2, 4, 8, 16],
        "array_sizes": [1000000, 4000000, 8000000, 16000000, 32000000, 64000000, 128000000]
    },
    "局部聚合": {
        "source": "sum_pthread_local.cpp",
        "binary": "sum_local_exec",
        "thread_counts": [1, 2, 4, 8, 16],
        "array_sizes": [1000000, 4000000, 8000000, 16000000, 32000000, 64000000, 128000000]
    }
}
```

### 表格展示

Mutex聚合 - 测试结果表格（单位：秒）

|线程数|1000000|4000000|8000000|16000000|32000000|64000000|128000000|
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
|1|0.000980|0.001229|0.003369|0.005663|0.010765|0.019471|0.036036|
|2|0.000447|0.001266|0.001979|0.005083|0.007087|0.011411|0.022210|
|4|0.001111|0.000797|0.001209|0.002038|0.005649|0.009104|0.017137|
|8|0.000527|0.001104|0.001298|0.002250|0.004222|0.008899|0.016757|
|16|0.001104|0.001178|0.002121|0.002757|0.004630|0.008222|0.012547|

---

局部聚合 - 测试结果表格（单位：秒）

|线程数|1000000|4000000|8000000|16000000|32000000|64000000|128000000|
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
|1|0.000681|0.002016|0.002434|0.005649|0.009303|0.021367|0.035084|
|2|0.000644|0.001288|0.002089|0.002744|0.005557|0.012817|0.020018|
|4|0.000488|0.000872|0.002547|0.003145|0.003717|0.008090|0.013478|
|8|0.000618|0.000937|0.001602|0.002920|0.003874|0.008013|0.014057|
|16|0.001002|0.001381|0.001640|0.002021|0.003969|0.008355|0.013333|


### 表格分析

1. **小数组（1M-8M）**：
   - 两种方法性能相近（<10%差异）
   - 局部聚合在单线程时略优（无锁开销）

2. **中等数组（16M-64M）**：
   - 局部聚合开始显现优势（最高快15%）
   - Mutex版本在高线程数时性能下降明显

3. **大数组（128M）**：
   - 局部聚合全面占优（最高快30%）
   - 16线程时优势最明显

4. **线程扩展性**：
   - 局部聚合展现更好的多线程扩展性
   - Mutex版本在8线程后收益递减

关键结论：局部聚合方法在高线程数和大数据量时优势显著，而Mutex方法在小数据量时简单有效。