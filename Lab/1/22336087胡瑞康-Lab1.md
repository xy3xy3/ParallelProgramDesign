# 中山大学计算机院本科生实验报告

（2025学年春季学期）

课程名称：并行程序设计
批改人：

|实验| 使用MPI点对点通信方式实现并行通用矩阵乘法 |专业（方向）|计算机科学与技术 |
|---|---|---|---|
|学号|22336087 |姓名|胡瑞康 |
|Email|hurk3@mail2.sysu.edu.cn |完成日期|2025.3.19 |

## 代码介绍

本实验实现了基于 **MPI（Message Passing Interface）** 的 **并行矩阵乘法**，使用**点对点通信**方式（`MPI_Send`/`MPI_Recv`）对矩阵数据进行分发和收集，兼容矩阵行数无法整除进程数的情况。

### **代码结构**
- **`main.cpp`**: 并行矩阵乘法的核心代码，利用 MPI 进行数据分发、计算与收集。
- **`evaluate.py`**: 用于自动化测试，执行不同进程数、不同矩阵规模的计算，并记录运行时间。

### 矩阵 B 的初始化与广播

```cpp
if (rank == 0) {
    B = (double*)malloc(n * k * sizeof(double));
    srand(time(NULL));
    for (int i = 0; i < n * k; i++) {
        B[i] = (double)(rand() % 10);
    }
}
MPI_Bcast(B, n * k, MPI_DOUBLE, 0, MPI_COMM_WORLD);
```

进程 0 随机生成矩阵 `B`，然后广播给所有进程，**每个进程完整持有 `B` 矩阵数据**，以便进行本地计算。

### 矩阵 A 的分块与点对点分发

```cpp
if (rank == 0) {
    A = (double*)malloc(m * n * sizeof(double));
    for (int i = 0; i < m * n; i++) {
        A[i] = (double)(rand() % 10);
    }

    for (int i = 1; i < size; i++) {
        int rows_to_send = (i < remainder) ? (rows_per_proc + 1) : rows_per_proc;
        MPI_Send(A + i * rows_per_proc * n, rows_to_send * n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
    }

    memcpy(local_A, A, local_rows * n * sizeof(double));
} else {
    MPI_Recv(local_A, local_rows * n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
}
```

- 将矩阵 `A` 按行块分割，每个进程只负责部分行的数据。
- 使用 `MPI_Send`/`MPI_Recv` 实现按需发送对应行数据。
- 特别处理了无法整除进程数的情况。

### 计算矩阵乘法

```cpp
for (int i = 0; i < local_rows; i++) {
    for (int l = 0; l < n; l++) {
        double temp = local_A[i * n + l];
        for (int j = 0; j < k; j++) {
            local_C[i * k + j] += temp * B[l * k + j];
        }
    }
}
```

修改了循环顺序，将 `l` 放在最外层，`i` 放在第二层，`j` 放在最内层，这样可以减少缓存未命中，提高计算效率。

### 点对点收集结果

```cpp
if (rank == 0) {
    C = (double*)malloc(m * k * sizeof(double));
    memcpy(C, local_C, local_rows * k * sizeof(double));

    for (int i = 1; i < size; i++) {
        int rows_to_receive = (i < remainder) ? (rows_per_proc + 1) : rows_per_proc;
        MPI_Recv(C + i * rows_per_proc * k, rows_to_receive * k, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    }
} else {
    MPI_Send(local_C, local_rows * k, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
}
```

- 每个进程将 `local_C` 通过 `MPI_Send` 发送给进程 0。
- 主进程按分配顺序接收每段结果，并拼接成最终矩阵 `C`。

### **计算执行时间**
```cpp
MPI_Barrier(MPI_COMM_WORLD);
start = MPI_Wtime();
// 执行矩阵乘法
finish = MPI_Wtime();
MPI_Reduce(&local_time, &max_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);
```

使用 `MPI_Wtime` 记录时间，计算每个进程的执行时间，并通过 `MPI_Reduce` 取所有进程的 **最大耗时** 作为最终计算时间。


## 运行测试

正常的编译，运行命令
```shell
export OMPI_ALLOW_RUN_AS_ROOT=1
export OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
mpic++ main.cpp -o main
mpirun -np 4 ./main
```

为了方便测试结果记录，我写了`evaluate.py`脚本，用于自动输入与匹配运行时间并总结表格

```py
import subprocess
import re

# 编译 C++ 源码
compile_command = "mpic++ main.cpp -o main"
print("正在编译 C++ 程序...")
subprocess.run(compile_command, shell=True, check=True)

# 定义测试参数：进程数和矩阵规模（假设 m=n=k，即正方矩阵）
process_counts = [1, 2, 4, 8, 16]
matrix_sizes = [128, 256, 512, 1024, 2048]

# 用于存放测试结果，键为 (进程数, 矩阵规模)，值为耗时（秒）
results = {}

# 正则表达式模式，用于提取形如 "矩阵乘法计算耗时：0.123456 秒" 中的数字部分
pattern = re.compile(r"矩阵乘法计算耗时：([\d\.]+) 秒")

# 遍历所有组合进行测试
for p in process_counts:
    for size in matrix_sizes:
        # 构造输入：三个整数，均为矩阵规模（m, n, k）
        input_str = f"{size} {size} {size}\n"
        # --oversubscribe用来防止There are not enough slots available in the system to satisfy the 16 slots that were requested by the application
        command = f"mpirun --oversubscribe -np {p} ./main"
        print(f"\n运行测试：进程数 = {p}, 矩阵规模 = {size} x {size}")
        try:
            # 运行命令，同时传入输入字符串，捕获标准输出
            result = subprocess.run(command, input=input_str, text=True, shell=True, capture_output=True, check=True)
            output = result.stdout
            # 使用正则表达式匹配计算耗时
            match = pattern.search(output)
            if match:
                time_consumed = match.group(1)
            else:
                time_consumed = "N/A"
            results[(p, size)] = time_consumed
            print(f"测试结果：耗时 = {time_consumed} 秒")
        except subprocess.CalledProcessError as e:
            print(f"运行过程中发生错误：{e}")
            results[(p, size)] = "Error"

# 构造 Markdown 格式的结果表格
print("\n测试结果表格：\n")
header = "|进程数|"
for size in matrix_sizes:
    header += f"{size}|"
separator = "|" + " :-: |" * (len(matrix_sizes) + 1)
print(header)
print(separator)
for p in process_counts:
    row = f"|{p}|"
    for size in matrix_sizes:
        row += f"{results[(p, size)]}|"
    print(row)

```

## 结果展示

### 表格总结

|进程数|128|256|512|1024|2048|
| :-: | :-: | :-: | :-: | :-: | :-: |
|1|0.010381|0.088377|0.700413|5.529363|44.570371|
|2|0.005298|0.044211|0.348689|2.839320|22.808313|
|4|0.002873|0.021984|0.183779|1.525855|12.001745|
|8|0.001803|0.013555|0.103347|0.841628|6.742776|
|16|0.002158|0.010262|0.078286|0.609859|5.012189|

### 表格分析

**计算时间随进程数增加逐步减少**
- 随着进程数的增加，计算时间逐渐减少。这是并行计算的典型现象——**任务被分配到多个进程中，能够减少每个进程的计算负担**，因此整体计算时间减少。

  **例如**：
  - **128×128** 矩阵：从 1 个进程的 **0.010381 秒** 降到 16 个进程时 **0.002158 秒**，这表明随着进程数增加，计算时间减少了大约 5 倍。

  - **2048×2048** 矩阵：从 1 个进程的 **44.570371 秒** 降到 16 个进程的 **5.012189 秒**，加速比也达到了大约 **9 倍**。

**大规模矩阵计算加速效果更明显**
- **大矩阵**（如 2048×2048）相较于 **小矩阵**（如 128×128）的加速效果更明显。并行化在计算大规模矩阵时，能够显著减少计算时间，说明该程序的并行性比较好，能够充分利用更多进程进行计算。

  **例如**：
  - **128×128** 矩阵在 1 个进程时耗时 0.010381 秒，而在 16 个进程时为 **0.002158 秒**，减少了约 5 倍。
  - **2048×2048** 矩阵在 1 个进程时耗时 44.570371 秒，而在 16 个进程时为 **5.012189 秒**，减少了约 **9 倍**，表明随着矩阵规模的增大，计算加速效果更加明显。


## 优化方向讨论

### 在内存有限的情况下，如何进行大规模矩阵乘法计算？

**分块矩阵乘法 (Block Matrix Multiplication)**：

可以将大矩阵分割成若干个较小的子矩阵（或称为块）。

假设要计算 $C = A \times B$，可以将 $A$ 分割成 $A_{ij}$，将 $B$ 分割成 $B_{jk}$，那么 $C$ 中的每个块 $C_{ik}$ 可以通过以下公式计算：
$C_{ik} = \sum_{j} A_{ij} \times B_{jk}$

这样，每次只需要加载参与当前子块乘法和加法运算的几个小矩阵到内存中，计算完成后再将结果写回。

通过合理地选择块的大小，可以控制内存的使用量。

在并行计算中，不同的进程可以负责计算 $C$ 中不同的块，从而实现并行化。

---

**外存算法 (Out-of-Core Algorithms)**：

如果矩阵规模非常庞大，甚至连部分子块都无法完全放入内存时，需要考虑使用外存（如硬盘）。

外存算法会将部分数据存储在磁盘上，需要时再加载到内存中进行计算。这通常涉及到复杂的数据调度和缓存管理，以尽量减少磁盘 I/O 的开销，因为磁盘 I/O 的速度远低于内存访问。

例如，可以按行或列分批加载矩阵数据到内存中进行计算，并将中间结果写回磁盘，最后再合并得到最终结果。

---

### 如何提高大规模稀疏矩阵乘法性能？

**选择合适的稀疏矩阵存储格式：**

与稠密矩阵不同，稀疏矩阵通常只存储非零元素及其索引。

常见的存储格式包括：
- Coordinate List (COO)：存储 (行索引, 列索引, 值) 的三元组列表。简单直观，但不利于高效的算术运算。
- Compressed Sparse Row (CSR)：按行存储非零元素。使用三个数组：`values` 存储非零元素的值，`col_indices` 存储每个非零元素对应的列索引，`row_pointers` 存储每一行第一个非零元素在 `values` 和 `col_indices` 中的起始位置。CSR 格式非常适合按行进行的操作，例如矩阵-向量乘法。
- Compressed Sparse Column (CSC)：按列存储非零元素，与 CSR 类似，但按列组织。适合按列进行的操作。

在进行稀疏矩阵乘法时，选择合适的存储格式至关重要，它可以直接影响到算法的效率。

例如，计算 $C = A \times B$，如果 $A$ 是 CSR 格式，$B$ 是 CSC 格式，通常可以实现较高效的乘法。

---

**实现优化的稀疏矩阵乘法算法：**

传统的稠密矩阵乘法算法不适用于稀疏矩阵。需要设计专门针对稀疏矩阵的乘法算法，避免对零元素进行操作。

例如，在计算结果矩阵 $C$ 的某个元素 $C_{ij}$ 时，只需要考虑 $A$ 的第 $i$ 行和 $B$ 的第 $j$ 列中非零元素对应的位置。只有当 $A_{ik} \neq 0$ 且 $B_{kj} \neq 0$ 时，乘积才可能非零。

算法通常会遍历第一个矩阵的非零元素，并在第二个矩阵中查找相应的非零元素进行计算。